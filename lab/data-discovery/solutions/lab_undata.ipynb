{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36c02f30",
   "metadata": {},
   "source": [
    "## What is Pearson correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae37ecc2",
   "metadata": {},
   "source": [
    "Given any two random variables $X$ and $Y$ associated to a population, the **Pearson correlation** is:\n",
    "\n",
    "\n",
    "$\\rho_{XY} = \\Large\\frac{cov(X,Y)}{\\sigma_{X}\\sigma_{Y}} = \\frac{\\mathbb{E}[(X - \\mu_{X})(Y - \\mu_{Y})]}{\\sigma_{X}\\sigma_{Y}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ca05e",
   "metadata": {},
   "source": [
    "When dealing with a sample (i.e. when we have a finite set of records which capture only a portion of the whole population of intereset, like in almost every dataset), the Pearson correlation is instead identified with:\n",
    "\n",
    "$r_{XY} = \\Large\\frac{\\sum_{i}(x_{i} - \\bar{x})(y_{i} - \\bar{y})}{\\sqrt{\\sum_{i}(x_{i} - \\bar{x})^2} \\sqrt{\\sum_{i} (y_{i} - \\bar{y})^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538005a1",
   "metadata": {},
   "source": [
    "### Why do we need approximate methods to search across a large collection of tables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39bb4c9",
   "metadata": {},
   "source": [
    "Computing correlation is a **costly operation**: it involves to align two variables (i.e. _join_ two datasets, if the variables are taken from different sources) and compute the _standard deviation_ and their _correlation_.\n",
    "\n",
    "Doing these steps with a high number of datasets may be unfeasible with limited resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795292a1",
   "metadata": {},
   "source": [
    "The **Quadrant Count Ratio** schema approximates the Pearson correlation, and it's faster and easier to compute than it on large scale.\n",
    "\n",
    "However, it's also generally less accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e557df1c",
   "metadata": {},
   "source": [
    "## Use BLEND to search for correlated variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cfb297",
   "metadata": {},
   "source": [
    "### Load libraries and define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b28c446",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dc7ebd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_path = Path(\"..\", \"data\", \"undata\")\n",
    "\n",
    "data_path.absolute(), data_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3f349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_path = Path(\"..\", \"modules\")\n",
    "blend_module_path = modules_path.joinpath(\"BLEND\")\n",
    "\n",
    "sys.path.append(str(blend_module_path.resolve()))\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c8992",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = data_path.joinpath(\"index_blend.db\")\n",
    "data_lake_path = data_path.joinpath(\"data-lake\")\n",
    "queries_path = data_path.joinpath(\"queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8febc621",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d671c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from blend import BLEND\n",
    "from blend.utils import clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ec11f1",
   "metadata": {},
   "source": [
    "### Load the BLEND index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95ff9dd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "index = BLEND(db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe691f2",
   "metadata": {},
   "source": [
    "### Load the query dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a84ad",
   "metadata": {},
   "source": [
    "We have some datasets in the _query_ folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f62869",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "queries = sorted(os.listdir(queries_path))\n",
    "\n",
    "print('\\n\\n'.join(queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab6fcb",
   "metadata": {},
   "source": [
    "## Join-Correlation Search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30291660",
   "metadata": {},
   "source": [
    "Given a query dataset composed by two columns, _Kq_ and _Xq_, \n",
    "we need to identify datasets on which we can perform a join on key _Kc_ and that have a numerical column _Xc_ that is highly correlated with _Xq_.\n",
    "\n",
    "Just looking for columns with an high join-overlap doesn't address our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9f7b08",
   "metadata": {},
   "source": [
    "Define which dataset we want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f6b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_table_idx = 2\n",
    "query_table_name = queries[query_table_idx]\n",
    "\n",
    "qdf = pl.read_csv(os.path.join(queries_path, query_table_name))\n",
    "\n",
    "print(f\"Query dataset: {query_table_name}\")\n",
    "\n",
    "qdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072afa37",
   "metadata": {},
   "source": [
    "Define on which key and target columns we will perform the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9a62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_name = 'Value'\n",
    "\n",
    "target_column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630fd7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_column_name = 'Country or Area'\n",
    "key_column_name = 'Sub-region Name'\n",
    "# key_column_name = 'Region Name'\n",
    "\n",
    "key_column_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3051b64",
   "metadata": {},
   "source": [
    "### What's inside the key column?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d63fe25",
   "metadata": {},
   "source": [
    "When working with joins, correlations, ..., the granularity level choosen for the search affects the final results.\n",
    "\n",
    "In our geographical datasets, fine-grained searches at the country level yield different results compared to coarser-grained searches at sub-regional or regional level.\n",
    "\n",
    "In particular, regional granularity isn't really useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e3eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdf.get_column(key_column_name).unique().sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b2d220",
   "metadata": {},
   "source": [
    "### Perform a Correlation Search (based on QCR schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd9e355",
   "metadata": {},
   "source": [
    "Run a correlation search on the query dataset. \n",
    "\n",
    "First, we group it by the selected key column, which will be used to identify _joinable_ tables.\n",
    "\n",
    "Then the retrieved tables will be ranked by an _estimate_ of the Pearson correlation, called **Quadrant Count Ratio** (QCR) correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d22042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUP BY key_column + MEAN ON target_column\n",
    "grouped_qdf = qdf.group_by(key_column_name).agg(pl.col(target_column_name).mean())\n",
    "\n",
    "# rename, just because it will be useful in later steps\n",
    "grouped_qdf = grouped_qdf.rename({target_column_name: 'Value_left'})\n",
    "\n",
    "grouped_qdf.sort(key_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab5cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the key column values\n",
    "keys = grouped_qdf.get_column(key_column_name).map_elements(clean, pl.String).to_list()\n",
    "\n",
    "# extract the target column values\n",
    "targets = grouped_qdf.get_column('Value_left').to_list()\n",
    "\n",
    "keys[:3], targets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c85623",
   "metadata": {},
   "source": [
    "Run the correlation search task: see the relative SQL query used under the hood at **blend.Operators.Seekers.Correlation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f994078",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = index.correlation_search(keys, targets, 20)\n",
    "\n",
    "results_df = pl.DataFrame(results, schema=['dataset', 'join_col_idx', 'target_col_idx', 'QCR'], orient='row')\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf0ed3",
   "metadata": {},
   "source": [
    "### Compare results with actual Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f0e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "def compare_with_pearson(results: list) -> list:\n",
    "    results_with_pearson = []\n",
    "\n",
    "    # basically, for each record load the relative dataset and compute \n",
    "    # the exact Pearson correlation (we have all necessary information, \n",
    "    # from the column indexes for key and target columns to the actual dataset)\n",
    "    for table_id, join_col_idx, target_col_idx, qcr in results:\n",
    "        r_df = pl.scan_csv(data_lake_path.joinpath(f\"{table_id}.csv\"))\n",
    "\n",
    "        # aggregate each dataset on its identified key column \n",
    "        # and compute the mean of the group\n",
    "        r_df = r_df.group_by(pl.nth(join_col_idx)).agg(pl.nth(target_col_idx).mean()).collect()\n",
    "        \n",
    "        # rename the column (just to make simpler next steps)\n",
    "        target_col_name = r_df.columns[1]\n",
    "        r_df = r_df.rename({r_df.columns[0]: key_column_name, r_df.columns[1]: 'Value_right'})\n",
    "        \n",
    "        # join our query grouped dataframe with the retrieved one\n",
    "        # (which is, as well, grouped)\n",
    "        join = grouped_qdf.join(\n",
    "            r_df, \n",
    "            on=key_column_name\n",
    "        )\n",
    "\n",
    "        # extract the numerical columns used to compute the correlation\n",
    "        value_left = join.get_column('Value_left')\n",
    "        value_right = join.get_column('Value_right')\n",
    "\n",
    "        # it could happen that after aggregation an array is\n",
    "        # constant: in this case, Pearson correlation is not\n",
    "        # defined and in the end we will have to discard these\n",
    "        # NaN values...\n",
    "        statistics = stats.pearsonr(value_left, value_right)\n",
    "\n",
    "        pearson = statistics.correlation\n",
    "        p_value = statistics.pvalue\n",
    "\n",
    "        results_with_pearson.append(\n",
    "            [\n",
    "                table_id, join_col_idx, target_col_idx, target_col_name, qcr, pearson, p_value\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return results_with_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c44fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_pearson = compare_with_pearson(results)\n",
    "\n",
    "results_with_pearson_df = pl.DataFrame(\n",
    "    results_with_pearson, \n",
    "    schema=['dataset', 'join_col_idx', 'target_col_idx', 'target_col_name', 'QCR', 'pearson', 'p_value'], \n",
    "    orient='row'\n",
    "    ).with_row_index('rank')\n",
    "\n",
    "results_with_pearson_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e9f07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool, Slider, CustomJS\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.layouts import row\n",
    "\n",
    "# Display Bokeh plots in Jupyter\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddf1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize p-values\n",
    "def categorize_pval(p):\n",
    "    if p < 0.05:\n",
    "        return \"< 0.05\"\n",
    "    elif p < 0.5:\n",
    "        return \"0.05-0.5\"\n",
    "    else:\n",
    "        return \">=0.5\"\n",
    "\n",
    "df_pd = results_with_pearson_df.with_columns(\n",
    "    pl.col('p_value').map_elements(categorize_pval, pl.String).alias('p_category')\n",
    ").to_pandas()\n",
    "\n",
    "# Convert to Bokeh ColumnDataSource\n",
    "source = ColumnDataSource(df_pd)\n",
    "source_all = ColumnDataSource(df_pd)\n",
    "\n",
    "# Define color mapping for categories\n",
    "categories = [\"< 0.05\", \"0.05-0.5\", \">=0.5\"]\n",
    "colors = [\"red\", \"orange\", \"blue\"]\n",
    "\n",
    "# Create interactive plot\n",
    "p = figure(\n",
    "    title=\"Estimated vs Actual Pearson (interactive)\",\n",
    "    x_axis_label=\"Pearson\",\n",
    "    y_axis_label=\"Estimated Pearson\",\n",
    "    width=700,\n",
    "    height=700,\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset\"\n",
    ")\n",
    "\n",
    "# Add scatter with hover tool\n",
    "renderer = p.scatter(\n",
    "    x=\"pearson\",\n",
    "    y=\"QCR\",\n",
    "    source=source,\n",
    "    size=8,\n",
    "    legend_field=\"p_category\",\n",
    "    fill_alpha=0.7,\n",
    "    color=factor_cmap(\"p_category\", palette=colors, factors=categories)\n",
    ")\n",
    "\n",
    "# Add identity line y=x\n",
    "p.line([-1, 1], [-1, 1], line_dash=\"dashed\", line_color=\"black\")\n",
    "\n",
    "# Add hover tool\n",
    "hover = HoverTool(\n",
    "    renderers=[renderer],\n",
    "    tooltips=[\n",
    "        (\"Pearson\", \"@pearson{0.000}\"),\n",
    "        (\"Estimate\", \"@QCR{0.000}\"),\n",
    "        (\"p-value\", \"@p_value\"),\n",
    "        (\"p-category\", \"@p_category\"),\n",
    "        (\"target_name\", \"@target_col_name\"),\n",
    "        (\"dataset\", \"@dataset\"),\n",
    "        (\"rank\", \"@rank\")\n",
    "    ]\n",
    ")\n",
    "p.add_tools(hover)\n",
    "\n",
    "p.legend.title = \"p-value bins\"\n",
    "p.legend.location = \"top_left\"\n",
    "\n",
    "\n",
    "# Slider widget for filtering by rank\n",
    "slider = Slider(start=df_pd[\"rank\"].min(),\n",
    "                end=df_pd[\"rank\"].max(),\n",
    "                value=df_pd[\"rank\"].max(),\n",
    "                step=1,\n",
    "                title=\"Max rank in top-K\")\n",
    "\n",
    "# Reassign the ENTIRE data dict\n",
    "callback = CustomJS(args=dict(source=source, source_all=source_all, slider=slider), code=\"\"\"\n",
    "    const A = source_all.data;\n",
    "    const max_pos = slider.value;\n",
    "\n",
    "    const idx = [];\n",
    "    const n = A['rank'].length;\n",
    "    for (let i = 0; i < n; i++) {\n",
    "        if (A['rank'][i] < max_pos) idx.push(i);\n",
    "    }\n",
    "\n",
    "    function pick(key) { return idx.map(i => A[key][i]); }\n",
    "\n",
    "    source.data = {\n",
    "        pearson: pick('pearson'),\n",
    "        QCR: pick('QCR'),\n",
    "        p_value: pick('p_value'),\n",
    "        p_category: pick('p_category'),\n",
    "        target_col_name: pick('target_col_name'),\n",
    "        dataset: pick('dataset'),\n",
    "        rank: pick('rank'),\n",
    "    };\n",
    "\"\"\")\n",
    "\n",
    "slider.js_on_change(\"value\", callback)\n",
    "\n",
    "# Layout (plot + slider)\n",
    "layout = row(p, slider)\n",
    "\n",
    "print(f\"Query dataset: {query_table_name}\")\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5c5a31",
   "metadata": {},
   "source": [
    "The QCR scheme gives an approximation of the Pearson correlation between two numerical variables. However, before going on with any analysis, the p-value should be checked to verify that the correlation could be considered statistically significant, and not spurious.\n",
    "\n",
    "In general, this is a **pure statistical method**, and no information on the actual meaning of a variable is involved. Thus, a final check is always required before using the retrieved columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5078846",
   "metadata": {},
   "source": [
    "# Exercise: Compute Correlation on More than one key column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e231b",
   "metadata": {},
   "source": [
    "What if we need as key a combination of two or more elements? The QCR schema cannot address this scenario in its basic formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35262692",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_table_idx = 0\n",
    "query_table_name = queries[query_table_idx]\n",
    "\n",
    "qdf = pl.read_csv(os.path.join(queries_path, query_table_name))\n",
    "\n",
    "print(f\"Query dataset: {query_table_name}\")\n",
    "\n",
    "qdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ea7ed",
   "metadata": {},
   "source": [
    "We can run a join-correlation query on every key attribute and the aggregate the final results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bbcdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a join-correlation query against the \"Sub-region Name\" column\n",
    "key_column_name = \"Sub-region Name\"\n",
    "target_column_name = \"Value\"\n",
    "\n",
    "grouped_qdf = qdf.group_by(key_column_name).agg(pl.col(target_column_name).mean())\n",
    "grouped_qdf = grouped_qdf.rename({target_column_name: 'Value_left'})\n",
    "\n",
    "keys = grouped_qdf.get_column(key_column_name).map_elements(clean, pl.String).to_list()\n",
    "targets = grouped_qdf.get_column('Value_left').to_list()\n",
    "\n",
    "results_subregion = index.correlation_search(keys, targets, 20)\n",
    "results_df__subregion = pl.DataFrame(results_subregion, schema=['dataset', 'join_col_idx', 'target_col_idx', 'QCR'], orient='row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b209446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_pearson_df_subregion = pl.DataFrame(\n",
    "    compare_with_pearson(results_subregion), \n",
    "    schema=['dataset', 'join_col_idx', 'target_col_idx', 'target_col_name', 'QCR', 'pearson', 'p_value'], \n",
    "    orient='row'\n",
    "    ).with_row_index('rank').filter(pl.col(\"target_col_name\").is_in([\"Year\", \"Decade\"]).not_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d7632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a join-correlation query against the \"Decade\" column\n",
    "key_column_name = \"Decade\"\n",
    "target_column_name = \"Value\"\n",
    "\n",
    "grouped_qdf = qdf.group_by(key_column_name).agg(pl.col(target_column_name).mean())\n",
    "grouped_qdf = grouped_qdf.rename({target_column_name: 'Value_left'})\n",
    "\n",
    "keys = grouped_qdf.get_column(key_column_name).map_elements(clean, pl.String).to_list()\n",
    "targets = grouped_qdf.get_column('Value_left').to_list()\n",
    "\n",
    "results_decade = index.correlation_search(keys, targets, 20)\n",
    "results_df__decade = pl.DataFrame(results_decade, schema=['dataset', 'join_col_idx', 'target_col_idx', 'QCR'], orient='row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7307a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_pearson_df_subregion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed7b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_pearson_df_decade = pl.DataFrame(\n",
    "    compare_with_pearson(results_decade), \n",
    "    schema=['dataset', 'join_col_idx', 'target_col_idx', 'target_col_name', 'QCR', 'pearson', 'p_value'], \n",
    "    orient='row'\n",
    "    ).with_row_index('rank').filter(pl.col(\"target_col_name\").is_in([\"Year\", \"Decade\"]).not_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd55255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_pearson_df_decade.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c29d2",
   "metadata": {},
   "source": [
    "What are the datasets that appear in both result sets? What is the real Pearson correlation? \n",
    "\n",
    "If we instead join the datasets on the two columns and then compute the correlation, does this value\n",
    "differ from the previously computed ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c80b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_subregion = results_with_pearson_df_subregion.get_column(\"dataset\").to_list()\n",
    "datasets_decade = results_with_pearson_df_decade.get_column(\"dataset\").to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dfb83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_datasets = set(datasets_subregion).intersection(datasets_decade)\n",
    "common_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2942227",
   "metadata": {},
   "source": [
    "Otherwise, we can create a custom key on all the datasets for which we can recognize the desired key columns. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
